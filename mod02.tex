\chapter{Módulo 2: Evaluación de modelos de aprendizaje automático}

La  evaluación depende de la tarea que se realice. Distinguiremos entre tres tipos de tarea:

\begin{itemize}
    \item \tb{Aprendizaje supervisado}, donde tenemos variables de entrada y una variable de salida, que representa la solución deseada. La meta es aprender la \ti{regla general} que convierte los datos de entrada en la solución correcta. Distinguimos entre:
    \begin{itemize}
        \item \tb{Clasificación}, donde la salida es una categoría.
        \item \tb{Regresión}, la variable resultado es numérica.
    \end{itemize}
    \item \tb{Aprendizaje no supervisado}, no se asigna ninguna etiqueta al algoritmo de aprendizaje, sólo existen variables de entrada. Distinguiremos:
    \begin{itemize}
        \item Clustering
        \item Reglas de asociación, 
        \item Correlaciones
    \end{itemize}
    \item \tb{Aprendizaje de Refuerzo}, donde un programa informático interactúa con un ambiente controlado en el que debe alcanzar una meta concreta: conducir un vehículo o los videojuegos son ejemplos de este tipo de tarea. Muy utilizado en Inteligencia Artificial y/o Robótica.
\end{itemize}

\section{Métricas de Clasificación}

La clasificación se predice a partir de una serie de entradas, para obtener una variable que es \tb{categórica}:
\begin{itemize}
    \item puede tener dos o más valores, 
    \item no tiene un orden, puede ser si/no, a/b/c o d
\end{itemize} 
el modelo lo que intentará predecir a cuál de las clases pertenecen los datos. Lo que tenemos que establecer es que si comparamos la clase predicha y la real coinciden.

La clase predicha se denota como \funcc{h}{x} y la clase actual como \func{x}, donde $x$ es el ejemplo.
\begin{center}
    \includegraphics[scale=.65]{images/mod02-01.png}
\end{center}
En el ejemplo, el modelo desplegado, a través de los datos etiquetados como \ti{se ha comprado} o \ti{no se ha comprado} un producto, se comparan con los que predice el modelo. Vemos que en algunos casos no concuerdan, esos casos son \tb{errores}. En la clasificación lo que contamos es cuantas veces se falla, nos estamos refiriendo al \ti{error}; en el ejemplo las veces que falla la predicción son 3 sobre 10, el error es de \nperc{30} o \ti{0.3}. Podemos referirnos de forma inversa y decir que se acierta un \nperc{70} de las predicciones, lo que normalmente se llama \tb{porcentaje de acierto}, en inglés se desgina con el término \tb{accuracy}.

Este tipo de medida, porcentaje de acierto o porcentaje de error, es muy simple y en ocasiones se queda corta a la hora de entender como funciona realmente el modelo.

Una de las métricas habituales en clasificación son las que se conocen como:
\begin{itemize}
    \item \tb{Precission},  representa el porcentaje de documentos que son relevantes para la consulta, con \tb{TP} no referimos a \tb{True Positives}, \ti{valores positivos que han sido verdaderos}, en el ejemplo $TP/ \text{positivos predichos}$.
    ..
    \item \tb{Recall}, representa el porcentaje de documentos que se devuelven por el estudio o modelo (TP ), $TP/ \text{positivos reales}$.
\end{itemize}

Si queremos integrar estados medidas, que nos dé más información, podemos recurrir otro tipo de medida, también habitual que se denomina \tb{Medida F}, o también denominada \tb{media armónica}:

$$\text{Medida F} = 2\,\frac{\text{precission }\ast \text{ recall}}{\text{precission }+ \text{ recall}} = \frac{2TP}{2TP + FP + FN}$$

Un modelo con una \ti{Medida F} alta suele tener medidas altas en precission y recall.

\begin{description}
    \item[TP] o Positivos Predichos correctamente.
    \item[FP]  o Negativos Predichos como Positivos.
    \item[TN] o Negativos Predichos correctamente. 
    \item[FN] o Positivos Predichos como Negativos.  
    \item[TNRate] o porcentaje de negativos verdaderos 
\end{description}
estos parámetros nos permiten conocer el rendimiento de un modelo de clasificación binaria en circunstancias específicas.
\begin{center}
    \includegraphics[scale=.65]{images/mod02-02.png}
\end{center}


\section{Métricas para regresión}

Recordemos que los modelos de regresión predicen un valor numérico a partir de una serie de entradas, este valor numérico no es algo exacto, sino que tiene decimales, en ocasiones muchos. Puesto que utiliza gran cantidad de datos, éstos pueden contener \ti{«ruido»} que hace que sea difícil obtener un valor exacto para la variable de salida.

Si quisieramos predecir la edad de una persona es una tarea difícil, lo que nos interesa es que los valores que obtengamos no se alejen mucho del valor real. Para evaluar cuánto nos alejamos o acercamos a ese valor real existen difrentes aproximaciones, que vemos a continuación, y se fundamentan en el cálculo del \tb{valor absoluto}, entre \ti{valor real} y el \ti{valor predicho}:

\begin{description}
    \item[Error Medio Absoluto, \funcc{MAE_S}{h}], lo que se hace es evaluar si el valor real menos el predicho, valor absoluto, para cada uno de los ejemplos de la muestra.
    $$\funcc{MAE_S}{h} = \frac{1}{n}\,\sum_{n\in S}|\func{x} - \funcc{h}{x}|$$
    \item[Error Cuadrático Medio, \funcc{MSE_S}{h} ],  lo que queremos es penalizar más aquellos casos en los que el error es más grande, lo que desequilibra el valor real del error, donde hay ejemplos que tienen un error pequeño y otros que puede ser enorme; utilizamos el \tb{error cuadrático}, consiste en elevar al cuadrado la diferencia entre el valor real y el predicho para cada uno de los ejemplos y calculamos su media.
    $$\funcc{MSE_S}{h} = \frac{1}{n}\,\sum_{n\in S}(\func{x} - \funcc{h}{x})^2$$
    Con este cálculo un error de 5 unidades se transforma en 25, pero uno de 10 se convierte en 100.
    \item[Raíz Cuadrada del Error Cuadrático Medio, \funcc{RMSE_S}{h}] , es un método que se utiliza para ajustar el valor que obtuvimos con \funcc{MSE_S}{h} a algo más real, consiste en obtener la \tb{ráiz cuadrada}. En ocasiones se confunden estos dos últimos cálculos, y no se distingue cuál se utiliza hasta que no se ven los cálculos realizados.
    $$\funcc{RMSE_S}{h} = \sqrt{\funcc{MSE_S}{h}} = \sqrt{ \frac{1}{n}\,\sum_{n\in S}(\func{x} - \funcc{h}{x})^2 }$$
\end{description}

Veamos un ejemplo, tenemos valores predichos y actuales (reales) para diez ejemplos:
\begin{center}
    \includegraphics[scale=.65]{images/mod02-03.png}
\end{center}

Ignoraremos los millones y calculamos el \tb{error}, valor abosluto, como la diferencia entre valor actual y valor predicho, se suman todos estos valores obtenidos de error y se dividen por el número de ejemplos y obtendremos el valor de \funcc{MAE}{h} = 7. Por otra parte, el error cuadrático medio lo que hace es elevar al cuadrado cada uno de los valores obtenidos para el error, valor absoluto, y calcula la media, obteniendo un valor de \funcc{MSE_S}{h} de 74,4, como vemos al elevar al cuadrado, algunos de los errores son muy elevados, lo que dificulta la comparación ambos valores. Por último, para corregir esto recurrimos a calcular la ráiz cuadrada de \funcc{MSE_S}{h}, $\funcc{RMSE_S}{h} = \sqrt{74,4} = 8,63$.

¿Qué media se utiliza más?. En principio el \funcc{MSE_S}{h}, penaliza más los errores, más grandes, fijándonos en el caso de valor 400, es más de la mitad de la suma de todos los cuadráticos, no contribuye a la media de todo el error cuadrático, lo que penaliza la métrica muchísimo más.

Con el valor del \ti{error cuadrático medio} podemos ver los valores que detecta bien esos valores. Mediante la \ti{raíz cuadrada} obtenemos un valor expresado en términos de la magnitud.

Podemos tener un modelo en que los valores alrededor de mil, y casi todos los ejemplos son mil uno, novecientos noventa y nueve, \ldots, es decir, que si yo tengo un error de tres, porporcionalmente es pequeño, tres entre mil es muy pequeño, pero en realidad si los valores reales están entre mil uno, mil dos, novecientos noventa y nueve, el error que cometo es muy alto, la variabilidd que tiene esos datos es tan pequeña que el error es mucho más grande que la variabilidad o por eso normalmente las métricas de error relativo lo que hacen es dividir por algo que mida más o menos la variabilidad. La forma de medir esta variabilidad es utilizando las \tb{varianzas}:

\begin{description}
    \item[Error Cuadrático Medio Relativo]
    $$ \funcc{RSE_S}{h} = \frac{ \displaystyle{\sum_{n\in S}(\func{x} - \funcc{h}{x})^2 }}{\displaystyle{\sum_{x\in S}(\overline{f} - \func{x})^2}} $$
    \item[Error Absoluto Medio Relativo]  
    $$ \funcc{RAE_S}{h} = \frac{ \displaystyle{\sum_{n\in S}|\func{x} - \funcc{h}{x}|}}{\displaystyle{\sum_{x\in S}|\overline{f} - \func{x}|}} $$
\end{description}

Una medida fácil de calcular y que permite ver si el mdodelo es alto cuando el valor real es alto y bajo cuando el valor real es bajo, es calcular la correlación:
\begin{itemize}
    \item \tb{Correlación de Pearson}, que es la correlación lineal.
    \item \tb{Correlación de Spearman}, correlación de rangos o de orden.
\end{itemize}

\subsection{Métricas para Aprendizaje No Supervisado}

Una de las tareas más habituales en \ti{aprendizaje no supervisado} es el \tb{agrupamiento} o \tb{clustering}; lo que queremos obtener con el agrupamiento, donde tenemos un conjunto de datos y en el que no hay variables de salida, sólo de entrada, es separar los ejemplos que tienen relación entre sí y forman un grupo, de los que se diferencian y pertenecen a otros grupos. La idea es la diferencia o similitud en términos de distancia, se trata de una \ti{métrica de distancia}. Para ver que elementos son diferentes, establecemos si la distancia entre elementos es alta.

¿Cómo creamos estos grupos? . Podemos crear grupos donde los elementos que los integran estén muy cerca, a poca distancia, y que están muy lejos unos grupos de otros. Esto dependerá de la técnica que utilicemos para el agrupamiento, no todas las técnicas hacen el agrupamiento de la misma forma.

Por ejemplo, si utilizo una técnia de agrupamiento que me saca bordes en los grupos, lo que me puede interesar es establecer la distancia entre los \ti{«bordes de los clusters»} o entre los \ti{elementos más extremos}, pero puede interesarme establecer la distancia entre los centros de dos grupos para ver si están lejos o no. Por lo tanto, podemos mirar un \ti{centroid, el radio, lo grande que son los clusters,\ldots}, noes compleja la evaluación de elementos de los modelos de agrupamiento, pero hay muchas métricas, dependiendo de si queremos más separación, que estén mas compactos.

\begin{center}
    \includegraphics[scale=.65]{images/mod02-04.png}
\end{center}

\subsection{Sobreajuste}

Pensemos en un modelo de regresión en el que queramos evaluar la edad o las ventas dependiendo del mes,  si disponemos del \ti{error cuadrático medio} o el \ti{error absoluto}, lo que necesitamos es extraer el menor error posible. Es relativamente fácil, si disponemos de unos datos, ajustar esos datos de forma que minimizen el error, pero si luego lo utilizamos con otros datos, el mismo modelo, puede que no se ajuste igual de bien.

El típico ejemplo de \tb{sobreajuste}, que todos vivimos como estudiantes al estudiar un exámen anterior real, nos hemos ajustado a las preguntas que salian en él, pero cuando hacemos el exámen real, las preguntas han cambiado, y el resultado no ha sido lo bueno que esperabamos. Es un caso típico de \ti{sobreajuste} que aparece en el \ti{«aprendizaje humano»}, en el \ti{aprendizaje automático} aparece exáctamente los mismo; si entrenamos un modelo a partir de unos datos y lo evaluamos sobre esos datos es fácil engañarse, es muy fácil ajustarse a esos datos y cuando tengamos otros datos desconocidos fallemos. Podemos verlo reflejado gráficamente como:
\begin{description}
    \item[Over-fitting o sobreajuste], utilizamos todos los datos para entrenar y evaluar los modelos. Gráficamente uniriamos todos los puntos, como cuando hacemos un dibujo., pero no generalizamos en las zonas en las que estén cerca esos puntos
    \begin{center}
        \includegraphics[scale=.65]{images/mod02-05_1.png}
    \end{center}
    En la gráfica vemos un pico, que se correspondería con un error o porque los datos en esa zona tienen una cierta variabilidad e intentamos un ajuste a algo que realmente no es un patrón de datos. Lo que deberíamos haber hecho es seguir la línea y evitar el pico.
    \item[Under-fitting o subajuste], compensamos el modelo generalizándolo, el ejemplo sería \ti{podar un árbol de datos}
    \begin{center}
        \includegraphics[scale=.65]{images/mod02-05_2.png}
    \end{center}  
    Viendo la gráfica podríamos pensar que los datos tienen un cierto patrón o contínuo entre los puntos representados, que podría intentar ajustar, pero no lo hacemos. Hemos generalizado demasiado el modelo con lo que prácticamente no obtenemos ningún patrón.
\end{description}

Esta es la duda principal, cómo puedo resolver o distinguir entre ambos casos automáticamente o visualmente. Lo que haremos es lo que se denomina \tb{regla de oro de la evaluación}:
\begin{center}
    \tb{\ti{nunca usar el mismo ejemplo para entrenar el modelo y evaluarlo}}
\end{center}
La mejor forma de hacer esto es escoger un conjunto de datos y separarlos o disponer de dos conjuntos diferentes de datos y utilizar un conjunto para el entrenamiento y el otro para la evaluación. En la práctica lo que haremos srá coger los datos y los dividiremos entre datos de entranamiento y datos para test.
\begin{center}
    \includegraphics[scale=.65]{images/mod02-06.png}
\end{center}
¿Cuántos cogemos para entrenamiento y cuántos para el test? Variará dependiendo del experimento, pero suele ser $2/3$ para entrenamiento y $1/3$ para test, o $70:30$.

¿Cómo se hace esta separación?. No se hace en orden, sino  de forma aleatoria; no se hace en el orden en que se reciben los datos ya que pueden tener algún tipo de sesgo, se hace un muestreo aleatorio, donde extraemos el \nperc{70} de los datos para entrenamiento y el resto para el la evaluación.

En el \ti{entrenamiento} lo que hacemos es aplicar los \tb{algoritmos} y \tb{obtener modelos}, que constitiye la parte central del aprendizaje automático. Una vez obtenidos los modelos los \tb{evaluamos} con los datos de test, con ello podremos empezar a calcular las métricas o lo que sea necesario. En ese caso sabemos que si el modelo se hubiera ajustado a particularidades, que por casualidad han caído en los datos de entrenamiento o intenta ajustarse mucho al \ti{«ruido»} de esos datos, variabilidades que puedan presentar los datos, cuando se entra en la fase de test, eso se notará, el modelo tendrá peores resultados en la evaluación. Con eso podremos resolver el \ti{sobreajuste} y el \ti{subajuste}, al final la medida que tenemos que mirar es  la \ti{medida que nos den los datos de la evaluación}.

¿Y la \tb{cohesión}?. Hemos dividido los datos en la proporción \ti{70:30}, imaginemos que tengo cien datos, puedo pensar ¿por qué no entreno con los 100 datos?, si 100 datos son pocos y ahora los divido en dos grupos (\ti{70:30}), pierdo un \nperc{30} de datos en el entrenamiento y tan sólo utilizo 30 ejemplos para evaluar el modelo. La solución no es aumentar el porcentaje de datos para entrenamiento, ésto hace que siga disminuyendo el número de datos para evaluar el modelo, el balance entre datos de entrenamiento y de test es difícil de encontrar, sobre todo cuando el volumen de datos es pequeño.

Cuantos más ejemplos tengamos para entrenamiento y test mejor, pero siempre sin romper la \ti{regla de oro}. Si no queremos un entrenamiento y evaluación pobres, y tenemos pocos datos, la solución se basa en la \tb{repetición del experimento}, varias veces sin romper en cada uno de los experimentos la \ti{regla de oro}, y al final hacer \tb{promedios de los resultados} obtenidos en cada experimento. Hay bastantes  maneras de hacerlo, pero nos centramos en dos:
\begin{itemize}
    \item \tb{Bootstrap}, se cogen n-muestras con repeticiones y se realiza la evaluación con el resto, se obtienen ejemplos con repetición, un ejemplo puede salir varias veces, y luego se testea con el resto. No se sigue ningún orden.
    \item \tb{Validación cruzada}, particionamos los datos en lo que se llaman \tb{pliegues}, evaluamos con un pliegue y aprendemos con el resto de ejemplos.
\end{itemize}

\subsection{Validación Cruzada}

En un contexto donde tengamos muchos datos, el problema de los datos de entrenamiento y de evaluación no sería un problema, dispondríamos de datos suficientes para aplicar la \ti{regla de oro}, pero en los casos en que tenemos pocos datos (100, 200, \ldots) tenemos un dilema, al dividirlos en datos de entrenamiento y de validación, tendríamos pocos casos para ambos grupos, por lo tanto, hemos de intentar resolver este problema.

Existe una solución, que a base de aplicar repetición podemos conseguir no romper la \ti{regla de oro}. ¿Cómo conseguir este proceso mágico?. Es un proceso que se conoce como \tb{validación cruzada}, en principio, tenemos los datos que partiremos en \ti{n-pliegues},
\begin{center}
    \includegraphics[scale=.67]{images/mod02-07.png}
\end{center}
en la imagen vemos que se han creado 7 pliegues, quiere decir que por cada iteración tendremos siete iteraciones, en la imagen sólo vemos la primera y la última; en cada iteración cogeremos uno de los pliegues para test, evaluación del modelo, y el resto para entrenamiento, esto se hará siete veces. A continuación, lo que se hace es sencillo, cogemos los seis pliegues de entrenamiento, cada pliegue puede tener 10, 15, \ldots ejemplos, los que hayan salido y entrenar el modelo y, por último, evaluarlo con el pliegue destinado a test.

Supongamos que tras el proceso obtenemos un \nperc{93} en el primero de los pliegues. En el pliegue dos podríamos obtener un \nperc{92.5}, así con el resto de pliegues, y el último podría ser de \nperc{94.2}. Hacemos un promedio de esos valores, que podemos suponer que sea de \nperc{92.7}, con ese promedio, que es el de \ti{evaluación}, indicaría cómo se comporta el modelo. Lo siguiente sería determinar con cuál de los grupos (pliegues) nos quedamos, y a cuál de ellos asigno el promedio estimado. La respuesta es \tb{a ninguno}. Lo que haríamos, que no refleja la imagen, es escoger todos los datos y entrenamos el modelo con todos esos datos, el resultado, en este ejemplo que hemos supuesto que era de clasificación, es un \tb{árbol de decisión} que se comporta como la media de todos los otros modelos que entrenamos en su momento, que era \nperc{93.7}, lo que hacemos es quedarnos con el modelo que acabamos de entrenar, con todos los datos y le asignamos una evaluación de \nperc{93.7}. ¿Por qué establecemos ésta canlidad? Si nos fijamos, todos los modelos se han evaluado sólo con seis pliegues, pero el último modelo lo hemos generado con todos los pliegues, el conjunto de datos inicial, con lo que tenemos más datos, lo que hace que el modelo obtenido es mejor, suele ser una estimación más robusta y, además, conseguimos entrenar el modelo con todos los datos. Si nos fijamos, en ningún momento hemos evaluado con datos que se utilizaron para el entrenamiento.

Normalmente, en este método de \ti{variación cruzada} es habitual utilizar diez pliegues, pero depende del coste computacional, cada pliegue son repeticiones, si hay diez pliegues, son diez repeticiones más el modelo final, es decir, \ti{once procesos de aprendizaje}. Si se trata de una tarea costosa como puede ser una red neuronal, podemos limitar esos pliegues a cinco o cuatro, pero si no son muchos los datos, que es cuando realmente utilizamos la \ti{variación cruzada}, podemos irnos a 10, 20 o incluso hacer tantos pliegues como ejemplos, en el caso de que los recursos nos lo permitan y tengamos pocos ejemplos.

\begin{center}
    \includegraphics[scale=.67]{images/mod02-08.png}
\end{center}

Hemos visto que los modelos de \ti{clasificación} se suelen evaluar con métricas relacionadas con el \ti{porcentaje de acierto}, mientras que las tareas de regresión se evalúan con métricas relacionadas con el \ti{error absoluto medio} o el \ti{error cuadrático medio}. Lo que no sabemos es dónde falla exactamente nuestro modelo. Para ello, vamos a ver para la \ti{clasificación} y la \ti{regresión} dos formas de representar cómo se manifiestan esos errores, como se distribuyen:
\begin{itemize}
    \item \ti{Clasificación}, veremos la \tb{contingencia}, cómo se distribuyen los errores, también se denomina \tb{confusión} o \tb{matriz de confusión}.
    \begin{center}
        \includegraphics[scale=.67]{images/mod02-09.png}
    \end{center}
    Esta matriz que representa un problema de dos clases: comprar o no comprar, quiere predecir si un cliente compra o no compra un producto. Suponemos que tenemos diez ejemplos, de ellos, si vemos la parte superior (Actual) representa los valores reales, tendremos $4+2=6$ en la primera columna, seis casos reales en los que el cliente compró, y $1+3=4$ en los que no compró. Lo que queremos ver es si nuestro modelo \ti{predice bien o no}.
    \begin{enumerate}
        \item Podemos ver que realmente acierta siete veces, 4 se predicen como de compra y 3 como de no compra. Pero queremos saber su distribución, ¿cuántos de los que realmente se compró, van a parar a parar a no comprar y cuántos de los que no se compró, van a compró?, es decir, la otra diagonal de la tabla, la \ti{diagonal de los errores}. Podemos ver que de los 6 que se compraron, en 4 acierta el modelo y en dos falla, y de los que no compraron, 1 falla y 3 acierta; vemos cláramente como se distribuye. Vemos que de los 6 casos de comprar, acierta 4 y falla 2 que representa una tercera parte, y de los no compra, 4 ejemplos, falla 1 y acierta 3, una cuarta parte, por lo tanto, los errores parecen estar bien balanceados.
    \end{enumerate}
    \item \ti{Regresión}, en lugar de una matriz de 2,3, n-clases lo que tenemos es un número, en principio, infinito de valores y no podemos representarlos en una clase. Si predice $3.2$ y el valor real es $3.1$,  estamos bastante cerca, aunque si se hiciera como clasificación, serían casillas diferentes. Aquí lo que generaremos es un \tb{gráfico de dispersión} que representa en un eje el valor predicho y en el otro el actual (no importa en qué eje pongamos estos valores)
    \begin{center}
        \includegraphics[scale=.67]{images/mod02-10.png}
    \end{center}
    Si tenemos un valor predicho de 40 podemos ver el valor real correspondiente, siendo $\approx 20.xxx$, en este ejemplo indica que el valor real es mucho menor que el predicho. Los valores por encima de la diagonal del gráfico nos indican valores predichos que se han quedado más cortos que el valor real , mientras que los que están por debajo se han alejado más. Dicha diagonal representa un \tb{clasificador perfecto},  y nos servirá para ver los casos de \ti{sobreajuste} y \ti{subajuste}, tendremos una visión bastante clara de cómo se distribuyen nuestros errores, tanto en clasificación como regresión.
\end{itemize}

Con estos procesos podemos comparar incluso distintas matrices de confusión o de contingencia y distintos gráficos de dispersión.

\begin{center}
    \includegraphics[scale=.67]{images/mod02-11.png}
\end{center}

\subsection{Clasificación binaria: Matriz de confusión}

De una matriz de contingencia o de confusión de un clasificador podemos extraer cualquier tipo de medida.

Si tenemos esta matriz de confusión, en la que tenemos los valores reales, son las columnas, y los valores predichos son las filas, estos serían los positivos verdaderos, los positivos falsos, que son errores, por eso son falsos, los negativos verdaderos que son aciertos y los negativos falsos.
\begin{center}
    \begin{tabular}{ c c }
        \includegraphics[scale=.67]{images/mod02-12_1.png}    &
        \includegraphics[scale=.67]{images/mod02-12_2.png}    
    \end{tabular}
\end{center}
donde:
\begin{itemize}
    \item \tb{TP} serían los positivos verdaderos.
    \item \tb{FP} serían los positivos falsos, los errores.
    \item \tb{FN} serían los negativos falsos.
    \item \tb{TN} serían los negativos verdaderos, aciertos.
\end{itemize}
En el ejemplo de comprar-no comprar, la diagonal de aciertos se corresponde con los valores 4-3 o \ti{true positive, TP}, mientras que la diagonal 1-2 son los fallos o \ti{true negative, TN}. En cuanto a los porcentajes de aciertos o error se calcula mediante las dos fórmulas siguientes:
\begin{description}
    \item[Accuracy] : $\frac{TP + TN}{N}\implies \frac{4+3}{10} = 0.7$ o \nperc{70} de aciertos.
    \item[Error]: $\frac{FP+FN}{N} = 1 - \text{Accuracy }\implies\frac{2+1}{10} = 0.3$ o \nperc{30} de error.
\end{description}

Por lo tanto, las dos medidas más clásicas en evaluación de clasificación se derivan fácilmente de la \tb{diagonal de la matriz de confusión}.

Veamos otras medidas más complejas:
\begin{description}
    \item[Sensibilidad o TPRate] se representa como $\frac{TP}{TP+FN}$,
    \item[Especificidad o TNRate], se representa como  $\frac{TN}{FP+TN}$,
    \item[Precission o PPV (valor positivo predecible)] , se representa como  $\frac{TP}{TP+FP}$
\end{description}

Estas dos medidas son simétricas, son dos puntos de la diagonal de aciertos divididos por la suma de cada una de las respectivas columnas.

Lo que suele confundir es que \tb{sensibilidad}, que en inglés se conoce como \tb{recall}, es el \ti{TP} dividido por la primera columna , mientras que la \ti{precisión}, es \ti{TP} pero dividido la primera fila. Tanto \ti{recall} como presicisón tienen \ti{TP} en el numerador, pero una denominador difiere en uno de los sumandos. En algunos libros o ejemplos, la posición de \ti{actual} y \ti{predicción} estan cambiadas a como aparecen en la imagen, deberemos tener cuidado para determina correctamente los valores que deberemos utilizar en las fórmulas para realizar los cálculos correctos. Muchas de las herramienta que podemos utilizar calculan tanto \ti{recall} como \ti{precission}, pero es importante saber como  cada celda de la matriz de confusión afectan a estas medidas.

Por último, hay una medida, la que denominamos \ti{F-measure} que es la \tb{media armónica de esas dos medidas, TPRate y PPV}:

\[
\begin{array}{l}
    TP = a \\
    FN = b \\
    FP = c \\
    \\
    \text{TPRate} = \frac{a}{a + b}\\
    \text{PPV} = \frac{a}{a + c} \\
\end{array}    
\]
\begin{equation*}
    \begin{split}
        \text{TPRate}\ast\text{PPV} &= \frac{a}{a + b}\ast \frac{a}{a + c} \\
        &= \frac{a^2}{a^2 + ab + ac + bc} \\
        &= \frac{a^2}{a(a+b+c)+bc} \\
        \\
        \\
        \text{TPRate} + \text{PPV} &= \frac{a}{a + b} + \frac{a}{a + c} \\
        &= \frac{a(a+c) + a(a+b)}{(a+b)(a+c)} = \frac{a(a + c + a +b)}{a(a+b+c)+bc} \\
        &= \frac{a(2a + c + b)}{a(a+b+c)+bc} 
        \\
        \\
        \text{F-mesaure} &= 2\ast\frac{PPV\ast TPRate}{PPV + TPRate} \\
        &= 2 \ast \frac{a^2 / (a(a+b+c)+bc)}{a(2a+b
        c)(a(a+b+c)+bc)} \\
        &= 2\frac{a}{2a + b +c } = 2\frac{TP}{2TP + FP + FN}
    \end{split}
\end{equation*}

Estas medidas se aplican a \ti{clasificadores de dos clases}, es por lo que en la tabla vemos los signos \ti{+} y \ti{-} (positivos y negativos); ¿qué hacemos cuando tenemos más de dos clases?. Por ejemplo un problema como el siguiente:
\begin{center}
    \includegraphics[scale=.75]{images/mod02-13.png}
\end{center}
La \tb{matriz de confusión o contingencia} funciona del mismo modo que para dos clases, en este caso, para la clase real tenemos tres valores: bajo, medio y alto, y los mismos para la clase predicha. La \tb{diagonal descendente (porcentaje de aciertos, 20-15-60)}, el porcentaje de aciertos sería:
\[
\text{Porcentaje de aciertos} = \frac{20+15+60}{20+13+5+15+4+4+7+60} = \frac{95}{133} =   0.714286 \approx \nperc{71.4286}
\]
El error serían todos los valores que no están en esa diagonal, con lo que es fácil generalizar estas medidas, ya sean tres, cuatro, cinco o más. Simplemente la \ti{matriz de confusión será más grande}, pero la \tb{diagonal descendente} representará siempre los \tb{aciertos}. Lo que sí será más complejo es derivar medidas como la \ti{precission} co \ti{recall}, ahora no podríamos hablar de positivos o negativos, en estos casos, lo que se suele hacer, \ti{se elige una de las clases como la \tb{positiva}}. En nuestro ejemplo podría ser \ti{low}, agrupando como \tb{clase negativa} a \ti{medium y high}, con lo que estos valores se sumarían, transformándose en una matriz de \ti{2x2}, \ti{medium y high} pasarían a ser \ti{medium-high}. Lo hemos planteado para \ti{low}, pero podríamos haber tomado \ti{medium} o \ti{high} como clase \ti{positiva}. Si hacemos las tres combinaciones posibles y promediamos esos valores, obtendríamos una media en la que podríamos calcular cualquiera de estas métricas; se podrían hacer:
\begin{itemize}
    \item 1 contra todos, promedio de \ti{N} medidas parciales.
    \item 1 contra 1 (promedio  $N\ast(N-1)/2$ medidas parciales): low contra medium, low contra high y medium contra high. Tenemos estas tres combinaciones, pero si tuvieramos más clásis, aquí el numero de combinaciones sería mayor, se dispararía.
\end{itemize}
Son sólo \tb{medidas promedio}, podemos ver que en nuestro ejemplo, la clase \ti{high} tiene muchos más ejemplos que la clase \ti{medium}, por lo que podemos recurrir a opciones de promediar o ponerle pesos a distintas de esas medidas para calcular el valor medio.

Viendo la \ti{matriz de confusión} podemos analizar como mejorar nuestro modelo.
\begin{center}
    \includegraphics[scale=.75]{images/mod02-14.png}
\end{center}

\subsection{Datos no balanceados}

Uno de los problemas que se tienen en los problemas de clasificación es que no tenemos el mismo número de ejemplos en las dos clases. En la mayoría de los casos son situaciones \tb{no balanceadas}, tenemos muchos más de un ejemplo que de otro.

Podemos encontrar diferencias significativas en la proporción de datos entre clases. Por ejemplo, si tenemos un \nperc{99} de ejemplos de una clase y un \nperc{1} para la otra clase, en este caso la predicción será siempre de la clase mayoritaria, tendría un \nperc{99} de aciertos en la \ti{matriz de confusión}. 

Pero nos encontramos que con frecuencia \tb{la clase minoritaria} suelen ser las más importantes, que más nos interesa determinar cuando fallan, pueden ser la causa de problemas bastante serios y necesitaríamos disponer de un predictor para estos casos.

Estas situaciones desbalanceadas nos obligan a establecer un error muy bajo, ya que ignoran a esta clase minoritaria, priorizando a la mayoritaria. En estos casos podemos quere fijarnos en una única medida, la \tb{macroprecisión}\footnote{macroprecisión $\rightarrow$ \funcc{macroacc}{h}}, la medida clase por clase, nos mostraría que hay algo problemático en nuestra clasificación. Si tenemos que siempre acertamos con la clas mayoritaria, pero siempre fallamos con la minoritaria, al hacer el promedio por clases tendríamos un \nperc{50} de acierto de una clase perfecta y una clase desastrosa, lo que nos indicaría que nuestro clasificador no sirve para nuestro objetivo. 

Es un ejemplo en el que vemos que simplemente mirar medidas agregadas, muchas veces nos puede confundir y es mucho más útil, en general, mirar la matriz de confusión o la matriz de contingencia.
\begin{center}
    \includegraphics[scale=.75]{images/mod02-15.png}
\end{center}

\begin{center}
    \includegraphics[scale=.75]{images/mod02-16.png}
\end{center}

\subsection{Clasificadores duros y suaves}

Una forma avanzada de analizar los clasificadores es ver lo que predicen, podemos tener clasificadores que nos indiquen:
\begin{itemize}
    \item si tenemos varias clases, nos predigan a qué clase pertenece nuestro ejemplo.
    \item la probabilidad de que un ejemplo pertenezca a una de esas clases, o poner en orden la probabilidad de a qué clases es más probable que el ejemplo se ajuste, es lo que se conoce como \tb{clasificador suave}. 
\end{itemize}

Por lo tanto, un \tb{clasificador suave o \ti{«scoring»}} predice una clase, pero acompaña cada predicción con una \ti{estimación de la fiabilidad} de cada predicción. Por ejemplo, tenemos tres clases, un clasificador suave nos puede dar como resultados:
\begin{itemize}
    \item 7 para la primera clase.
    \item 2 para la segunda,
    \item 0.3 para la tercera
\end{itemize}
esto quiere decir que el valor más alto corresponde a la primera de las class, el algoritmo de clasificación estará más seguro de que pueda ser de la primera clase.

Los clasificadores suaves no proporcionan más posibilidades de funcionar y mejorar nuestras predicciones que los \tb{clasificadores duros o \ti{«crisp»}}.

\subsubsection*{Probabilidades y decisiones}

Una \tb{probabilidad} se puede convertir en \tb{decisiones} mediante el uso de un \tb{umbral}. Por ejemplo, un médico puede dar un medicamento a un paciente sólo si la probabilidad de que sea efectivo sea mayor de el \nperc{80}, mientras que otro médico puede establecer el umbral en el \nperc{60}.

Si un modelo asignara el \nperc{70} de probabilidad, con el primer umbral no se le daría el medicamento, pero con el segundo sí. Vemos que con el mismo modelo suave, variando el umbral, podemos tener diferentes decisiones.

No es necesario que el \tb{valor suave} sea realmente una probabilidad en el rango de \ti{0 y 1 (\nperc{0} y \nperc{100})}, ocurre lo mismo con cualquier otro valor numérico, como puede ser la \tb{confianza}.

\begin{center}
    \includegraphics[scale=.75]{images/mod02-17.png}
\end{center}

\subsection{Clasificadores probabilísticos}

Como vimos, los \ti{clasificadores suaves} no nos dan directamente la clase que queremos estimar, sino un valor de qué clase tiene una fiabilidad mayor de ser correcta.

Un tipo de \ti{clasificador suave} es el denominado \tb{estimador probabilístico de clase}, un sistema al que se asignan unas entradas sobre el ejemplo, pero en lugar de predecir la clase nos da la \tb{probabilidad para cada una de las clases u opciones}. Si tenemos tres clases (\ti{a,b} y \ti{c}), obtendremos la probabilidad para cada una de ellas: \mat{p_a},  \mat{p_b} y \mat{p_c}.

Digamos que tenemos dos clasificadores, \mat{clasificador_1} y \mat{clasificador_2} a los que asignamos los mismo valores de entrada, obtenemos los siguientes resultados:
\begin{itemize}
    \item \mat{clasificador_1}: \mat{p_a = 0.2}, \mat{p_b = 0.5} y \mat{p_c = 0.3}.
    \item \mat{clasificador_2}: \mat{p_a = 0.3}, \mat{p_b = 0.4} y \mat{p_c = 0.3}.
\end{itemize}
ambos predicen mejores resultados para el \ti{clase ,b}; si nuestro umbral a la hora de determinar que predecimos, simplemente es elegir la clase con la mayor probabilidad, ésta corresponde al  \mat{clasificador_1} obtiene mejor resultado; si tuvieramos que determinar que clasificador está más seguro de la predicción, en principio, tmabién sería el \mat{clasificador_1}, es más confiable pues se corresponde con el \nperc{50}, mientras que el segundo nos da el \nperc{40}. El segundo clasificador habría sido más \tb{conservador}, y si al final la clase correcta no fuera la \ti{b}, el segundo clasificador habría cometido un \ti{error menor} que el primero.

¿Cómo podemos evaluar lo buenas que son las estimaciones de nuestro ejemplo?. Necesitamos medidas que nos evalúen lo buenas o acertadas que son esas probabilidades.

Los \ti{clasificadores probabilísticos} predicen una probabilidad, tenemos un ejemplo y denominamos la \ti{probabilidad para el ejemplo i sobre la clase j}, obtendremos una definición que sirve para problemas de dos, tres o cuatro clases, que determinará o estimará la probabilidad para el ejemplo \ti{i} y la clase \ti{j}, y vamos a ver el valor real para el ejemplo \ti{i} y la clase \ti{j}. Si la clase del ejemplo \ti{i} es la \ti{clase j} su valor será 1, y si no será 0, en cambio, las probabilidades pueden ir entre 0 y 1.
\begin{equation*}
    \mat{MSE} = \frac{1}{n}\displaystyle{\sum_{i\in S}\sum_{j\in C}[\func{i,j} - \funcc{p}{i,j}]^2}
\end{equation*}
Aplicando la fórmula del \ti{error cuadrático medio (\mat{MSE})}, tendremos que en la parte izquierda, valores reales ($\func{i,j}$) sólo habrá valores 0 o 1, mientra que en la parte de predicción ($\funcc{p}{i,j}$) tendrá valores que pueden ir en el rango de 0 a 1. Si $\func{i,j} = 1$, es decir, la \ti{clase es la \tb{j}} y la probabilidad estimada $\funcc{p}{i,j} = 1$ para esa clase, es perfecto, estaríamos diciendo que para las otras clases la probabilidad estimada sería 0. Igual ocurre si los dos valores anteriores son 0, establece que la probabilidad de que sea la clase j es nula.

Por lo tanto, si no es esa clase, sea 0, y obtengamos un valor que no sea 0, será un error y todo lo que nos alejemos de 1 y nos deviemos desde el 1, que sería predecir que sí va a ser esa clase, también estamos desviandonos de la posibilidad perfecta. Es lo que hace la diferencia entre \mat{f} y \mat{p}, con el error cuadrático medio nos aseguramos de quitar un posible signo negativo, al estar elevada al cuadrado, por lo tanto el resultado siempre será positivo.

En el claso de los clasificadores probabilísticos el \tb{error cuadrático medio} también se denomina \tb{Brier Score}, anunque la definición es la misma, se denomina así porque este valor sólo puede ser \tb{1} o \tb{0}. Ésta es la medida más habitual para evaluar lo bueno que es un \ti{clasificador probabilístico}.

Otra medida bastante habitual, relacionda con la \ti{entropía} es lo que denominamos \tb{Log Loss}, simplemente es operar con los mismos valores , donde el valor real que se multiplica por el \tb{logaritmo} de la probabilidad, si los valores son similares darán valores bajos y si son diferentes, valores altos. Cuando más altos sean los valores, peor será el clasificador.

\begin{equation*}
    \mat{Logloss} = -\frac{1}{n}\displaystyle{\sum_{i\in S}\sum_{j\in C}(\func{i,j}\ast log_{2}\,\funcc{p}{i,j})}
\end{equation*}

Tanto \ti{Brier Score} y el \ti{Log loss} incluyen una medida que es un estimador de probabilidades. En ocasiones no sabemos si un estimador es bueno porque la clasificación  que hacemos con él es buena, o porque realmente las probabilidades están bien estimadas. Podemos tener un clasificador que siempre nos de $0.5$ de probabilidad para todas las clases, suponiendo que es un problema binario, en principio, si tenemos sólo dos clases, si tenemos un \nperc{50} de ejemplos para dos clases, podríamos ver si la estimación se decanta por una clase o mucho por la otra; podría estar bien calibrado o balanceado, pero el sistema no servía para nada ya que siempre estima un \nperc{50} para cada clase, no discrimina entre ejemplos.

Para determinar si en un modelo las probabilidades están bien calibradas y si el modelo realmente refina bien entre las dos clases utiliza las descomposición, para el \ti{Brier Score}, entre \ti{medidas de calibración} y \ti{medidas de refinamiento}. Cuando analizamos la información entre \ti{calibración} y \ti{refinamiento} podemos observar que a veces, sin modificar el modelo, podremos mejorar la \ti{estimación de probabilidad}; podemos tener medias que estén muy cerca de $0.5$ o muy extremas, lo que hacemos es un \ti{postproceso} al clasificador, calibrándolo mejor y mejorando la estimación de probabilidades.

\begin{center}
    \includegraphics[scale=.75]{images/mod02-18.png}
\end{center}

\subsection{Rankers}

Es otro tipo de \ti{clasificador suave}, nos proporciona un valor y se suele aplicar a problemas de clasificación de dos clases, en éstos tenemos que el clasificador estimará un valor, un \ti{scort} para cada uno de los ejemplos. Cuanto más alto sea el valor de \ti{scort}, mayor será la probabilidad de en ese ejemplo sea positiva, y cuanto más bajo que sea de la clase negativa.

Si cogemos todos los ejemplos de un \ti{dataset} lo que tenemos es un \tb{ranking}, donde los que están más arriba son los más probables de ser positivos, y los que están abajo, más probable que sean negativos.

El valor predicho por el \ti{ranker} not tiene por qué ser una probabilidad, simplemente es un valor más alto cuanto más probable es y viceversa. Lo que nos interesa es evaluar lo bueno que es ese ranking. Si tengo veinte unidades de un producto que quiero vender a mil clientes, como sólo tengo veinte unidades lo que me interesa saber, de los mil clientes, quienes tienen mayor probabilidad de compra de esps productos, con lo que puede que realmente necesite hacer una campaña sólo, por ejemplo, para un \nperc{10} de los clientes que tengan mayor probabilidad de compra, muchos no comprarán aunque tenga una mayor probabilidad, ya que la compra dependerá de otros muchos factores. Por lo tanto, lo que nos interesa saber es el \ti{ranking} y no tanto la \ti{probabilidad}, esto está relacionado con las medidas de discriminación entre dos clases y lo que es el refinamiento, cuando se habla de estimación de probabilidad; esta parte del refinamiento es lo que realmente evaluamos al hablar de \tb{ranker}.

En resumen, un \tb{ranker} es un \ti{clasificador suave} que da un \ti{valor monótamente relacionado con la probabilidad de clase uno}. Cuanto más alto sea mayor probabilidad de que sea la clase uno, clase positiva, y cuanto más bajo mayor probabilidad de que sea la clase cero, o negativa.

El \ti{ranker} es muy utilizado en \ti{marketing} a la hora de recomendar productos, lo que se hace es escoger un segmento de aquellos items que están más altos en el ranking, un percentil (\nperc{5} o el \nperc{10}) y sobre ellos se hace la campaña, de ellos puede que contesten un subconjunto, pero hemos limitado la campaña a aquellos con mayor probabilidad. Lo que nos interesa es ordenar todos los clientes y escoger los que nos interesan. Cuanto mayor sea el ranking, jmejor será la segmentación.

¿Cómo evaluamos lo bueno que es un ranker? Lo que más se utiliza es el \tb{área bajo la curva ROC (Receiver Operating Characteristic)}, normalmente se utilizan las siglas inglesas,\mat{AUC} (\ti{Area Under the ROC Curve}). Normalmente esta medida es equivalente a una estadística conocida como \ti{estadística de Willcoxon-Mann Whitney}, que se define como: \ti{dado un ejemplo psitivo y un ejemplo negativo, la probabilidad de que el modelo coloque en el ranking de estimación al ejemplo positivo por encima del ejemplo negativo}. Por ejemplo, tenemos un conjunto de ejemplos de entrenamiento, con sus valores de clase positivos y negativos, cogemos al azar un ejemplo positivo, igual hacemos con los ejemplos negativos, comprobamos si nuestro \tb{ranker} pone el positivo arriba del negativo; la probabilidad de que eso pase es el AUC. La forma de definirlo es simplemente realizar un conteo de cuántas veces, eligiendo al azar entre positivos y negativos, se confirma dicha ordenación. Esta métrica nos dice lo bueno que es nuestro ranking, un valor de 1 será perfecto, mientras que 0 será un desastre, lo normal es que el ranking esté en torno al \ti{0.5}.

Existe otras distancias para evaluar el ranking, por ejemplo, la distancia entre el \ti{ranking perfecto} y el \ti{ranking estimado}; esa discrepancia entre rankings, por ejemplo la denominada \tb{discrepancia de Kendall}, suele ser equivalente al área bajo la curva ROC, por lo que muchas veces sólo se utiliza ésta para evaluar rankers.

\begin{center}
    \includegraphics[scale=.55]{images/mod02-19.png}
\end{center}

\subsection{Evaluación sensible al coste}

